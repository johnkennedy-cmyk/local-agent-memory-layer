# LAML Services Docker Compose Configuration
#
# IMPORTANT: The LAML MCP Server for Cursor runs OUTSIDE Docker
# because Cursor spawns it via stdio. This docker-compose runs:
# - The HTTP API bridge (for the dashboard)
# - The dashboard itself
#
# Prerequisites:
# - Firebolt Core running on localhost:3473
# - Ollama running on localhost:11434

version: '3.8'

services:
  # LAML HTTP API Bridge (for dashboard metrics)
  laml-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: laml-api
    restart: always
    ports:
      - "8082:8082"
    environment:
      # Firebolt Core connection (running on host)
      - FIREBOLT_USE_CORE=true
      - FIREBOLT_CORE_URL=http://host.docker.internal:3473
      - FIREBOLT_DATABASE=laml
      
      # Ollama connection (running on host)
      - OLLAMA_HOST=http://host.docker.internal:11434
      - OLLAMA_MODEL=llama3:8b
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
      - OLLAMA_EMBEDDING_DIMENSIONS=768
    command: ["python", "-m", "src.http_api"]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # LAML Dashboard (React monitoring UI)
  laml-dashboard:
    image: node:20-alpine
    container_name: laml-dashboard
    restart: always
    working_dir: /app
    volumes:
      - ../dashboard:/app
      - dashboard_node_modules:/app/node_modules
    ports:
      - "5174:5174"
    environment:
      - VITE_API_URL=http://localhost:8082
    command: ["sh", "-c", "npm install && npm run dev -- --host 0.0.0.0"]
    depends_on:
      - laml-api

volumes:
  dashboard_node_modules:

networks:
  default:
    name: laml-network
